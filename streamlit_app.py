#TODO


import streamlit as st
st.set_page_config(layout="wide")
import pandas as pd
import numpy as np
import os
import PyPDF2
from pdf2image import convert_from_bytes
import gc
import pytesseract
#pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
from io import BytesIO
import openai
from openai import OpenAI
import tiktoken
import re

import traceback, sys
import gc

def global_exception_handler(exc_type, exc_value, exc_traceback):
    st.error("Unhandled exception:")
    st.code("".join(traceback.format_exception(exc_type, exc_value, exc_traceback)))

sys.excepthook = global_exception_handler


MODEL_PRICES = {
    "gpt-4o": {"input": 2.50, "output": 10.00},
    "gpt-4.1": {"input": 1.25, "output": 10.00},
    "gpt-4.1-mini": {"input": 0.25, "output": 2.00},
    "gpt-4.1-nano": {"input": 0.05, "output": 0.40},
}

def make_arrow_compatible(df: pd.DataFrame) -> pd.DataFrame:
    """Fixes DataFrame columns so they are Arrow-compatible for Streamlit."""
    if df is None or df.empty:
        return df
    df_fixed = df.copy()
    for col in df_fixed.columns:
        # minden object t√≠pus√∫ oszlopot stringg√© er≈ëltet√ºnk
        if df_fixed[col].dtype == "object":
            df_fixed[col] = df_fixed[col].astype(str)
    return df_fixed


def df_ready(df, required_cols=None):
    """Ellen≈ërzi, hogy a df nem √ºres √©s (opcion√°lisan) tartalmazza a k√∂telez≈ë oszlopokat."""
    if not isinstance(df, pd.DataFrame) or df is None or df.empty:
        return False
    if required_cols:
        return all(col in df.columns for col in required_cols)
    return True

def need_msg(missing_list):
    bullets = "\n".join([f"- {m}" for m in missing_list])
    st.warning(f"Az √∂sszef≈±z√©shez a k√∂vetkez≈ëk hi√°nyoznak vagy √ºresek:\n{bullets}")

def count_tokens(text, model="gpt-4o"):
    encoder = tiktoken.encoding_for_model(model)
    tokens = encoder.encode(text)
    return len(tokens)

def replace_successive_duplicates(df, column_to_compare, columns_to_delete):
    result = df.copy()
    col = column_to_compare
    mask = result[col] == result[col].shift()
    for col in columns_to_delete:
        result.loc[mask, col] = np.nan
    return result

def extract_text_from_pdf(uploaded_file):
    import cv2
    from PIL import Image

    file_name = uploaded_file.name
    pdf_content = ""

    # 1) sima sz√∂vegkinyer√©s
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        for page in pdf_reader.pages:
            pdf_content += page.extract_text() or ""
    except Exception as e:
        st.error(f"Hiba a(z) {file_name} f√°jl olvas√°sakor: {e}")
        return None

    # 2) OCR, ha t√∫l kev√©s sz√∂veg van
    if len(pdf_content.strip()) < 100:
        pdf_content = ""
        try:
            uploaded_file.seek(0)
            file_bytes = uploaded_file.read()

            # determine number of pages
            num_pages = len(PyPDF2.PdfReader(BytesIO(file_bytes)).pages)

            progress = st.progress(0)
            for i in range(1, num_pages + 1):
                # higher DPI for sharper OCR
                images = convert_from_bytes(file_bytes, dpi=300, first_page=i, last_page=i)
#                images = convert_from_bytes(file_bytes, dpi=300, first_page=i, last_page=i, poppler_path = r"C:\poppler-24.08.0\Library\bin") #local)

                # --- OpenCV preprocessing with Otsu threshold ---
                img = cv2.cvtColor(np.array(images[0]), cv2.COLOR_RGB2GRAY)
                _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

                # back to PIL for pytesseract
                img_pil = Image.fromarray(img)

                # OCR with Hungarian language, PSM 6 (single uniform block of text)
                custom_config = r'--psm 3'
                text = pytesseract.image_to_string(img_pil, lang="hun", config=custom_config)
                pdf_content += text + "\n"

                # mem√≥riatiszt√≠t√°s
                del images, img, img_pil
                gc.collect()

                progress.progress(i / num_pages)

        except Exception as e:
            st.error(f"OCR hiba a(z) {file_name} f√°jln√°l: {e}")
            return None

    # 3) hosszkorl√°toz√°s
    if len(pdf_content) > 300000:
        st.warning(file_name + " t√∫l hossz√∫, csak az els≈ë 300000 karakter ker√ºl feldolgoz√°sra.")
        pdf_content = pdf_content[:300000]

    return pdf_content





def generate_gpt_prompt(text, file_name):
    """Generates a clear, structured GPT prompt for invoice data extraction."""
    return (
        f"You are given the extracted text of a Hungarian invoice PDF file named '{file_name}'. "
        "The PDF may contain multiple invoices merged together. "
        "Your task is to extract the following **9 data fields** for each invoice:\n\n"
        "1. Seller name (string)\n"
        "2. Buyer name (string)\n"
        "3. Invoice number (string)\n"
        "4. Invoice date (string, e.g. '2024.04.01')\n"
        "5. Total gross amount (integer)\n"
        "6. Total net amount (integer)\n"
        "7. VAT amount (integer)\n"
        "8. Currency (string: 'HUF' or 'EUR')\n"
        "9. Exchange rate (integer, use 1 if invoice is in HUF)\n\n"
        "10. Comments (string), any text that describes the business transaction the invoice contains (max 100 characters). If there is no comment, write an empty string.\n\n"
        "**Important formatting instructions:**\n"
        "- Use semicolon (`;`) to separate the 10 fields.\n"
        "- Use **one line per invoice**.\n"
        "- Do **not** include field numbers (e.g. '1)', '2)' etc.) in the output.\n"
        "- Write all numeric fields as plain integers (e.g. `1500000`).\n"
        "- **Do not use thousands separators** (e.g. `.`) or decimal commas (`,`) in the output and only output the integer part of the numbers.\n"
        "- Note: In Hungarian, decimal separators are commas (`,`) instead of dots (`.`) and thousand separators are dots (`.`)\n"
        "- The invoice number often appears in or matches the file name. Always first try to extract the invoice number from the text itself, but you can compare it to the file name too.\n"
        "- Do **not** include any explanation, headings, or extra text ‚Äî just the data rows.\n\n"
        "Extracted text:\n"
        f"{text}"
    )


def extract_data_with_gpt(file_name, text, model_name):
    """A kiv√°lasztott GPT modellel kinyeri a strukt√∫r√°lt adatokat a PDF sz√∂vegb≈ël."""
    gpt_prompt = generate_gpt_prompt(text, file_name)

    # --- Debug: show extracted text ---
#    with st.expander(f"üìú Kinyert sz√∂veg ‚Äì {file_name}"):
#        st.text_area("Extracted text", gpt_prompt[:10000], height=300)  # first 10k chars


    try:
        client = OpenAI(api_key=openai.api_key)
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": ""},
                {"role": "user", "content": gpt_prompt}],
            #max_completion_tokens=5000,
            timeout=30
        )

        raw_output = response.choices[0].message.content.strip()
        rows = raw_output.split("\n")

        parsed_rows = []
        for row in rows:
            parts = row.strip().split(";")
            if len(parts) != 10:
                st.warning(f"Hib√°s sor ({file_name}): {row}")
                continue

            cleaned_parts = [re.sub(r"^\s*\d+\)\s*", "", p.strip()) for p in parts]
            parsed_rows.append([file_name] + cleaned_parts)

        return parsed_rows, count_tokens(gpt_prompt)

    except Exception as e:
        st.error(f"A {model_name} feldolgoz√°s sikertelen volt: {file_name} ‚Äì {e}")
        return [], 0



def normalize_number(value):
    """Converts numeric-looking strings or floats to int. Removes all formatting."""
    try:
        if pd.isna(value):
            return None
        if isinstance(value, (int, float)):
            return int(round(value))
        # Remove thousands separators ('.' or ',' or space), allow decimals
        cleaned = str(value).replace(" ", "").replace(",", "").replace(".", "")
        return int(cleaned)
    except:
        return None


def compare_with_tolerance(val1, val2, tolerance=500):
    try:
        val1 = normalize_number(val1)
        val2 = normalize_number(val2)

        # Ha b√°rmelyik hi√°nyzik ‚Üí "No Data"
        if val1 is None or val2 is None or pd.isna(val1) or pd.isna(val2):
            return "Nincs adat"

        return "Igen" if abs(val1 - val2) <= tolerance else "Nem"
    except Exception:
        return "Nincs adat"

    
    

def get_minta_amount(row, huf_col="√ârt√©k", eur_col="√ârt√©k deviza", currency_col="Devizanem"):
    """Returns the value in correct currency column based on Devizanem."""
    try:
        dev = str(row[currency_col]).strip().upper()
        if dev == "EUR":
            return normalize_number(row[eur_col])
        else:
            return normalize_number(row[huf_col])
    except:
        return None




def merge_with_minta(df_extracted, df_minta, invoice_col_extracted="Sz√°mlasz√°m", invoice_col_minta="Bizonylatsz√°m"):
    df_merged = pd.merge(df_minta, df_extracted, how='outer', left_on=invoice_col_minta, right_on=invoice_col_extracted)
    matched = df_merged[invoice_col_extracted].notna().sum()
    total = len(df_minta)
    unmatched = total - matched
    match_rate = round(100 * matched / total, 2)
    
    stats = {
        "√ñsszes minta sor": total,
        "Tal√°latok sz√°ma": matched,
        "Hi√°nyz√≥ tal√°latok": unmatched,
        "Egyez√©si ar√°ny (%)": match_rate
    }

    return df_merged, stats



# Inicializ√°ljuk a session state v√°ltoz√≥kat
if 'extracted_text_from_invoice' not in st.session_state:
    st.session_state.extracted_text_from_invoice = []
if 'extracted_data' not in st.session_state:
    st.session_state.extracted_data = []
if 'df_extracted' not in st.session_state:
    st.session_state.df_extracted = pd.DataFrame()
if 'df_minta' not in st.session_state:
    st.session_state.df_minta = pd.DataFrame()
if 'df_nav' not in st.session_state:
    st.session_state.df_nav = pd.DataFrame()
if 'df_karton' not in st.session_state:
    st.session_state.df_karton = pd.DataFrame()
if 'df_merged' not in st.session_state:
    st.session_state.df_merged = pd.DataFrame()
if 'df_merged_full' not in st.session_state:
    st.session_state.df_merged_full = pd.DataFrame()
if 'number_of_tokens' not in st.session_state:
    st.session_state.number_of_tokens = 0

openai.organization = "org-i7aicv7Qc0PO4hkTCT4N2BqR"
openai.api_key = st.secrets['openai']["OPENAI_API_KEY"]

st.title("üìÑ Sz√°mlaadat-kinyer≈ë alkalmaz√°s")

col_pdf, col_excel = st.columns([1, 1])  # nagyobb bal oldali has√°b

with col_pdf:
    st.subheader("üìÇ PDF-ek kinyer√©se")
    
    st.write("1) T√∂lts fel egy vagy t√∂bb **magyar nyelv≈± sz√°ml√°t (PDF)**, amelyekb≈ël a rendszer kiolvassa a legfontosabb adatokat.")
    
    # 0) F√°jl felt√∂lt≈ë
    uploaded_files = st.file_uploader("üì§ PDF f√°jlok felt√∂lt√©se", type=["pdf"], accept_multiple_files=True)
    
    asd = """selected_model = st.selectbox(
        "V√°lassz modellt az adatkinyer√©shez:",
        ["gpt-4o", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"],
        index=0,
        help="√Årak per 1M token:\n"
             "- gpt-4o: \\$2.50 input / \\$10 output\n"
             "- gpt-4.1: \\$1.25 input / \\$10 output\n"
             "- gpt-4.1-mini: \\$0.25 input / \\$2 output\n"
             "- gpt-4.1-nano: \\$0.05 input / \\$0.40 output"
        )"""
    
    selected_model = "gpt-4o"
    
    # 1) PDF feldolgoz√°s
    if st.button("üìë Adatkinyer√©s a PDF-b≈ël"):  
        st.session_state.extracted_text_from_invoice = []      
        if uploaded_files:
            if len(uploaded_files) > 200:
                st.write("‚ö†Ô∏è Az els≈ë 200 f√°jl ker√ºl feldolgoz√°sra.")
            files_to_process = uploaded_files[:200]
    
            # ---- 1) PDF sz√∂veg kinyer√©s progress bar ----
            pdf_progress = st.progress(0, text="PDF sz√∂veg kinyer√©se folyamatban...")
            pdf_status = st.empty()  # helyfoglal√≥ a st√°tusznak
    
            for idx, uploaded_file in enumerate(files_to_process, start=1):
                file_name = uploaded_file.name
                pdf_status.write(f"{file_name} feldolgoz√°sa (PDF sz√∂veg kinyer√©se)...")
                pdf_text = extract_text_from_pdf(uploaded_file)
    
                if pdf_text is None:
                    continue
    
                st.session_state.extracted_text_from_invoice.append([file_name, pdf_text])

    
                # update progress
                pdf_progress.progress(idx / len(files_to_process), 
                                      text=f"PDF kinyer√©s: {idx}/{len(files_to_process)} k√©sz")
    
            pdf_progress.empty()
            pdf_status.write("‚úÖ PDF sz√∂veg kinyer√©s befejezve.")
    
        else:
            st.warning("‚ö†Ô∏è K√©rlek, t√∂lts fel legal√°bb egy PDF f√°jlt.")
    
        # ---- 2) GPT adatkinyer√©s progress bar ----
        st.session_state.extracted_data = []
        if st.session_state.extracted_text_from_invoice:
            gpt_progress = st.progress(0, text="AI adatkinyer√©s folyamatban...")
            gpt_status = st.empty()  # helyfoglal√≥ a st√°tusznak
    
            for idx, (file_name, pdf_content) in enumerate(st.session_state.extracted_text_from_invoice, start=1):
                gpt_status.write(f"{file_name} feldolgoz√°sa (AI adatkinyer√©s)...")
                extracted_rows, tokens = extract_data_with_gpt(file_name, pdf_content, selected_model)
                if extracted_rows:
                    st.session_state.extracted_data.extend(extracted_rows)
                    st.session_state.number_of_tokens += tokens
    
                # update progress
                gpt_progress.progress(idx / len(st.session_state.extracted_text_from_invoice),
                                      text=f"AI feldolgoz√°s: {idx}/{len(st.session_state.extracted_text_from_invoice)} k√©sz")
    
            gpt_progress.empty()
            gpt_status.write("‚úÖ AI adatkinyer√©s befejezve.")
    
        # ---- 3) DataFrame l√©trehoz√°s ----
        if st.session_state.extracted_data:
            st.session_state.df_extracted = pd.DataFrame(
                st.session_state.extracted_data,
                columns=["F√°jln√©v", "Elad√≥", "Vev≈ë", "Sz√°mlasz√°m", "Sz√°mla kelte", 
                         "Brutt√≥ √°r", "Nett√≥ √°r", "√ÅFA", "Deviza", "√Årfolyam", "Megjegyz√©sek"]
            )
            st.session_state.df_extracted["Sz√°mlasz√°m"] = st.session_state.df_extracted["Sz√°mlasz√°m"].astype(str)


    
    if len(st.session_state.df_extracted) > 0:        
        st.write("‚úÖ **Adatok kinyerve!** Az al√°bbi t√°bl√°zat tartalmazza az eredm√©nyeket:")
        st.dataframe(make_arrow_compatible(st.session_state.df_extracted))
    
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            st.session_state.df_extracted.to_excel(writer, sheet_name='Adatok', index=False)
    
        # üîë reset pointer
        buffer.seek(0)
    
        st.download_button(
            label="üì• Kinyert adatok let√∂lt√©se Excelben",
            data=buffer,
            file_name='kinyert_adatok.xlsx',
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )


    # Token √°r becsl√©s
    price_input = st.session_state.number_of_tokens * MODEL_PRICES[selected_model]["input"] / 1_000_000
    st.write(f"üí∞ A becs√ºlt feldolgoz√°si k√∂lts√©g eddig: **${price_input:.2f}** ({selected_model})")

        
    import requests
    import xml.etree.ElementTree as ET
    import datetime
    
    def get_mnb_eur_rate(date_str, max_lookback_days=7, debug=False):
        """
        Gets the EUR/HUF rate from MNB SOAP webservice for the given date.
        Falls back to earlier dates if rate not found.
        """
        soap_url = "http://www.mnb.hu/arfolyamok.asmx"
        soap_action = "http://www.mnb.hu/webservices/MNBArfolyamServiceSoap/GetExchangeRates"
        headers = {
            "Content-Type": "text/xml; charset=utf-8",
            "SOAPAction": soap_action,
        }
    
        dt = datetime.datetime.strptime(date_str, "%Y-%m-%d")
    
        for i in range(max_lookback_days + 1):
            check_date = (dt - datetime.timedelta(days=i)).strftime("%Y-%m-%d")
    
            # SOAP request body
            soap_body = f"""<?xml version="1.0" encoding="utf-8"?>
            <soap:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                           xmlns:xsd="http://www.w3.org/2001/XMLSchema"
                           xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/">
              <soap:Body>
                <GetExchangeRates xmlns="http://www.mnb.hu/webservices/">
                  <startDate>{check_date}</startDate>
                  <endDate>{check_date}</endDate>
                  <currencyNames>EUR</currencyNames>
                </GetExchangeRates>
              </soap:Body>
            </soap:Envelope>"""
    
            debug = False
            if debug:
                st.code(soap_body, language="xml")
    
            try:
                resp = requests.post(soap_url, headers=headers, data=soap_body, timeout=10)
                resp.raise_for_status()
    
                if debug:
                    st.text_area(f"SOAP response for {check_date}", resp.text[:2000], height=150)
    
                # Parse XML response
                root = ET.fromstring(resp.content)
                ns = {"mnb": "http://www.mnb.hu/webservices/"}
    
                rate_text = root.find(".//mnb:GetExchangeRatesResult", ns)
                if rate_text is None or not rate_text.text:
                    continue
    
                # inner XML (string)
                inner_xml = ET.fromstring(rate_text.text)
                rate = inner_xml.find(".//Rate")
                if rate is not None and rate.text:
                    return float(rate.text.replace(",", "."))
    
            except Exception as e:
                if debug:
                    st.warning(f"Attempt {i+1} ({check_date}) failed: {e}")
                continue
    
        st.warning(f"‚ùå Nem tal√°lhat√≥ MNB √°rfolyam {date_str} vagy az el≈ëz≈ë {max_lookback_days} napban.")
        return None

    rate = get_mnb_eur_rate("2024-10-15", debug=True)
    print(rate)

    
    # --- üí∂ √Årfolyam ellen≈ërz√©s blokk ---
    if not st.session_state.df_extracted.empty:
        df_fx_check = st.session_state.df_extracted.copy()
    
        # sz≈±r√©s: csak EUR √©s √Årfolyam != 1
        df_fx_check = df_fx_check[
            (df_fx_check["Deviza"].str.upper() == "EUR")
        ].copy()
    
        if not df_fx_check.empty:
            st.markdown("### üí∂ √Årfolyam ellen≈ërz√©s (MNB hivatalos √°rfolyam alapj√°n)")
    
            # NEW: Debug toggle
            #debug_fx = st.checkbox("üîß Show API request/response debug", value=False, help="Print the full URL and raw XML/HTML response for each query/attempt.")
    
            if st.button("üîç √Årfolyamok ellen≈ërz√©se"):
                mnb_rates = []
                rate_matches = []
    
                for _, row in df_fx_check.iterrows():
                    try:
                        date_str = str(row["Sz√°mla kelte"]).strip()
                        parsed_date = pd.to_datetime(date_str, errors="coerce")
                        if pd.isna(parsed_date):
                            mnb_rates.append(None)
                            rate_matches.append("Nincs adat")
                            continue
    
                        mnb_date = parsed_date.strftime("%Y-%m-%d")
                        # pass debug flag through
                        mnb_rate = get_mnb_eur_rate(mnb_date, debug=False)
                        mnb_rates.append(mnb_rate)
    
                        if mnb_rate is not None:
                            cmp = compare_with_tolerance(row["√Årfolyam"], mnb_rate, tolerance=1)
                            rate_matches.append(cmp)
                        else:
                            rate_matches.append("Nincs adat")
    
                    except Exception:
                        mnb_rates.append(None)
                        rate_matches.append("Nincs adat")

    
                df_fx_check["MNB √°rfolyam"] = mnb_rates
                df_fx_check["√Årfolyam egyezik?"] = [x.replace('Igen', '‚úÖ Igen').replace('Nem', '‚ùå Nem') for x in rate_matches]
    
                # Statisztika
                total_fx = len(df_fx_check)
                matched_fx = sum(1 for x in rate_matches if x == "Igen")
                match_rate = round(100 * matched_fx / total_fx, 2) if total_fx else 0.0
    
                st.write(f"**Ellen≈ërz√∂tt EUR sz√°ml√°k sz√°ma:** {total_fx}")
                st.write(f"**Egyez≈ë √°rfolyamok:** {matched_fx}  ({match_rate}%)")
    
                st.dataframe(make_arrow_compatible(df_fx_check[[
                    "F√°jln√©v", "Sz√°mlasz√°m", "Sz√°mla kelte", "Deviza",
                    "√Årfolyam", "MNB √°rfolyam", "√Årfolyam egyezik?"
                ]]))
    
                # Excel let√∂lt√©s opci√≥
                buffer = BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                    df_fx_check.to_excel(writer, sheet_name='Arfolyam_ellenorzes', index=False)
                buffer.seek(0)
    
                st.download_button(
                    label="üì• √Årfolyam ellen≈ërz√©s let√∂lt√©se Excelben",
                    data=buffer,
                    file_name='arfolyam_ellenorzes.xlsx',
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
    


with col_excel:
    
    st.subheader("üìÇ Excel f√°jlok bet√∂lt√©se")
    
    
    # --- Excel f√°jlok bet√∂lt√©se ---
    st.markdown("1) T√∂ltsd fel a **Mintav√©tel** Excel f√°jlt:")
    uploaded_excel_file_minta = st.file_uploader(
        "üì§ Mintav√©tel Excel felt√∂lt√©se",
        type=["xlsx"],
        accept_multiple_files=False,
        help="Az adatok az els≈ë munkalapon a 10. sort√≥l induljanak, √©s legyen 'Bizonylatsz√°m' nev≈± oszlop."
    )
    
    
    if uploaded_excel_file_minta:
        try:
            st.session_state.df_minta = pd.read_excel(uploaded_excel_file_minta, skiprows=range(1, 9))
            st.session_state.df_minta.columns = list(st.session_state.df_minta.iloc[0])
            st.session_state.df_minta = st.session_state.df_minta.iloc[1:]
            st.session_state.df_minta["Bizonylatsz√°m"] = st.session_state.df_minta["Bizonylatsz√°m"].astype(str)
        except:
            st.warning("‚ùå Nem siker√ºlt beolvasni a Mintav√©tel f√°jlt.")
    
    if len(st.session_state.df_minta) > 0:        
        st.write("‚úÖ **Mintav√©tel bet√∂ltve!** Els≈ë n√©h√°ny sor:")
        st.dataframe(make_arrow_compatible(st.session_state.df_minta.head(5)))
    
    
    
    # NAV f√°jl
    st.markdown("2) T√∂ltsd fel a **NAV** Excel f√°jlt:")
    uploaded_excel_file_nav = st.file_uploader(
        "üì§ NAV Excel felt√∂lt√©se",
        type=["xlsx"],
        accept_multiple_files=False,
        help="Az adatok az els≈ë munkalapon a 6. sort√≥l induljanak, √©s legyen 'sz√°mlasorsz√°m' nev≈± oszlop."
    )    
    if uploaded_excel_file_nav:
        try:
            st.session_state.df_nav = pd.read_excel(uploaded_excel_file_nav, skiprows=5)
            st.session_state.df_nav["sz√°mlasorsz√°m"] = st.session_state.df_nav["sz√°mlasorsz√°m"].astype(str)
        except:
            st.warning("‚ùå Nem siker√ºlt beolvasni a NAV f√°jlt.")
    
    if len(st.session_state.df_nav) > 0:        
        st.write("‚úÖ **NAV f√°jl bet√∂ltve!** Els≈ë n√©h√°ny sor:")
        st.dataframe(make_arrow_compatible(st.session_state.df_nav.head(5)))
    
    # Karton f√°jl
    st.markdown("3) T√∂ltsd fel a **Karton** Excel f√°jlt:")
    uploaded_excel_file_karton = st.file_uploader(
        "üì§ Karton Excel felt√∂lt√©se",
        type=["xlsx", "xls"],
        accept_multiple_files=False,
        help="Az adatok az els≈ë munkalapon az A1 cell√°t√≥l induljanak."
    )    
    
    if uploaded_excel_file_karton:
        try:
            # Simply read the Excel file, no special column assumptions
            st.session_state.df_karton = pd.read_excel(uploaded_excel_file_karton)
    
            st.write("‚úÖ **Karton bet√∂ltve!** Els≈ë n√©h√°ny sor:")
            st.dataframe(make_arrow_compatible(st.session_state.df_karton.head(5)))
    
        except Exception as e:
            st.warning(f"‚ùå Nem siker√ºlt beolvasni a Karton f√°jlt: {e}")
    
     
st.title("üìÑ Ellen≈ërz√©sek")

col_left, col_right = st.columns([1, 1])  # nagyobb bal oldali has√°b

with col_left:
    st.subheader("üìé Kinyert adatok √∂sszef≈±z√©se √©s ellen≈ërz√©se: Mintav√©tel")
    
    invoice_colname_minta = "Bizonylatsz√°m"
        
    if st.button("üîó √ñsszef≈±z√©s √©s ellen≈ërz√©s a Mintav√©tel excellel"):
        # El≈ëfelt√©telek
        missing = []
        if not df_ready(st.session_state.df_extracted, required_cols=["Sz√°mlasz√°m", "Nett√≥ √°r"]):
            missing.append("Kinyert adatok (PDF feldolgoz√°s)")
        if not df_ready(st.session_state.df_minta, required_cols=["Bizonylatsz√°m", "√ârt√©k", "√ârt√©k deviza", "Devizanem"]):
            missing.append("Mintav√©tel Excel")
    
        if missing:
            need_msg(missing)
        else:
            try:
                df_minta = st.session_state.df_minta.copy()
                # Fejl√©cek biztons√°gos tiszt√≠t√°sa (nem haszn√°l .str-t)
                df_minta.columns = [str(c).strip() for c in df_minta.columns]
                df_minta["Bizonylatsz√°m"] = df_minta["Bizonylatsz√°m"].astype(str)
    
                df_gpt = st.session_state.df_extracted.copy()
                df_gpt["Sz√°mlasz√°m"] = df_gpt["Sz√°mlasz√°m"].astype(str)
                    
                # --- √°tnevez√©s suffix-szel ---
                df_gpt = df_gpt.add_suffix("_ai")
                df_minta = df_minta.add_suffix("_minta")
                
                # ‚¨ÖÔ∏è teljes outer merge ‚Äì hogy a csak-AI sorok is beker√ºljenek
                df_merged_minta = pd.merge(
                    df_minta,
                    df_gpt,
                    how="outer",
                    left_on="Bizonylatsz√°m_minta",
                    right_on="Sz√°mlasz√°m_ai",
                    indicator=True
                )
                
                # Tal√°lati st√°tusz
                status_map = {
                    "both": "üîó Egyez√©s",
                    "left_only": "üìÑ Csak Mintav√©tel",
                    "right_only": "ü§ñ Csak AI (PDF)"
                }
                df_merged_minta["Tal√°lat st√°tusz"] = df_merged_minta["_merge"].map(status_map)
                
                # Nett√≥ √∂sszehasonl√≠t√°s
                df_merged_minta["Nett√≥ egyezik?"] = df_merged_minta.apply(
                    lambda row: compare_with_tolerance(
                        get_minta_amount(
                            row,
                            huf_col="√ârt√©k_minta",
                            eur_col="√ârt√©k deviza_minta",
                            currency_col="Devizanem_minta"
                        ),
                        normalize_number(row.get("Nett√≥ √°r_ai")),
                        tolerance=5
                    ),
                    axis=1
                )
                
                # Minden egyezik?
                df_merged_minta["Minden egyezik?"] = df_merged_minta["Nett√≥ egyezik?"].map({
                    "Igen": "‚úÖ Igen",
                    "Nem": "‚ùå Nem",
                    "Nincs adat": ""
                })
                
                # --- RENDEZ√âS: ---
                
                # c√©l: fel√ºl az egyez√©sek (both), k√∂z√©pen a csak mint√°k (left_only), alul a csak AI (right_only)
                status_order = pd.CategoricalDtype(categories=["both", "left_only", "right_only"], ordered=True)
                df_merged_minta["_merge"] = df_merged_minta["_merge"].astype(status_order)
                
                # jel√∂lj√ºk, ha teljesen egyezik a nett√≥ √©rt√©k
                df_merged_minta["__ok_order"] = df_merged_minta["Minden egyezik?"].eq("‚úÖ Igen").astype(int)
                
                # üîß most a kateg√≥ria sorrend garant√°lja a helyes sorrendet
                df_merged_minta = (
                    df_merged_minta
                    .sort_values(
                        by=["_merge", "__ok_order"],
                        ascending=[True, False]  # Egyez√©s fel√ºl, "‚úÖ Igen" el≈ër√©bb
                    )
                    .drop(columns=["_merge", "__ok_order"])
                    .reset_index(drop=True)
                )

                
                st.session_state.df_merged_minta = df_merged_minta
                
                # --- statisztika ---
                total = len(df_merged_minta)
                matched = (df_merged_minta["Minden egyezik?"] == "‚úÖ Igen").sum()
                match_rate = round(100 * matched / total, 2) if total else 0.0
                
                st.session_state.stats_minta = {
                    "√ñsszes sor a t√°bl√°ban": total,
                    "Minden egyez√©s": matched,
                    "Egyez√©si ar√°ny (%)": match_rate
                }
                
                st.success("‚úÖ √ñsszef≈±z√©s √©s rendez√©s k√©sz ‚Äî az egyez√©sek fel√ºl, csak mint√°k k√∂z√©pen, csak AI alul!")

    
            except Exception as e:
                st.error(f"V√°ratlan hiba t√∂rt√©nt a Mintav√©tel √∂sszef≈±z√©s sor√°n: {e}")

    
    if "df_merged_minta" in st.session_state:
        st.write("üìÑ **√ñsszef≈±z√∂tt √©s ellen≈ërz√∂tt t√°bl√°zat ‚Äì Mintav√©tel:**")
        st.dataframe(make_arrow_compatible(st.session_state.df_merged_minta))
    
        csv_minta = st.session_state.df_merged_minta.to_csv(index=False).encode("utf-8")
    
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            st.session_state.df_merged_minta.to_excel(writer, sheet_name='Minta', index=False)
    
        # üîë Reset buffer pointer to the start
        buffer.seek(0)
    
        st.download_button(
            label="üì• Let√∂lt√©s Excel (Mintav√©tel)",
            data=buffer,
            file_name='merged_minta.xlsx',
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    
        st.markdown("### üìä Statisztika ‚Äì Mintav√©tel ellen≈ërz√©s")
        for k, v in st.session_state.stats_minta.items():
            st.write(f"**{k}:** {v}")


with col_right:
    st.subheader("üìé Kinyert adatok √∂sszef≈±z√©se √©s ellen≈ërz√©se: NAV")
    
    if st.button("üîó √ñsszef≈±z√©s √©s ellen≈ërz√©s a NAV excellel"):
        # El≈ëfelt√©telek
        missing = []
        if not df_ready(st.session_state.df_extracted, required_cols=["Sz√°mlasz√°m", "Brutt√≥ √°r", "Nett√≥ √°r", "√ÅFA"]):
            missing.append("Kinyert adatok (PDF feldolgoz√°s)")
        nav_required = ["sz√°mlasorsz√°m"]
        if not df_ready(st.session_state.df_nav, required_cols=nav_required):
            missing.append("NAV Excel")
    
        if missing:
            need_msg(missing)
        else:
            try:
                # NAV adat el≈ëk√©sz√≠t√©s
                df_nav = st.session_state.df_nav.copy()
                df_nav.columns = [str(c).strip() for c in df_nav.columns]
                df_nav["sz√°mlasorsz√°m"] = df_nav["sz√°mlasorsz√°m"].astype(str)
                
                # GPT adat el≈ëk√©sz√≠t√©s
                df_gpt = st.session_state.df_extracted.copy()
                df_gpt["Sz√°mlasz√°m"] = df_gpt["Sz√°mlasz√°m"].astype(str)
                
                # --- √°tnevez√©s suffix-szel ---
                df_gpt = df_gpt.add_suffix("_ai")
                df_nav = df_nav.add_suffix("_nav")
                
                # NAV aggreg√°l√°s sz√°mlasz√°m szinten
                agg_dict = {}
                for col in ["brutt√≥ √©rt√©k_nav", "brutt√≥ √©rt√©k Ft_nav", "nett√≥√©rt√©k_nav", "nett√≥√©rt√©k Ft_nav", "ad√≥√©rt√©k_nav", "ad√≥√©rt√©k Ft_nav"]:
                    if col in df_nav.columns:
                        agg_dict[col] = "sum"
                df_nav_sum = df_nav.groupby("sz√°mlasorsz√°m_nav", as_index=False).agg(agg_dict)
                
                # √ñsszef≈±z√©s sz√°mlasz√°m szint≈± √∂sszehasonl√≠t√°shoz
                df_check = pd.merge(
                    df_gpt,
                    df_nav_sum,
                    how="left",
                    left_on="Sz√°mlasz√°m_ai",
                    right_on="sz√°mlasorsz√°m_nav"
                )
                
                # Oszlopnevek rugalmas keres√©se
                brutto_col = next((c for c in ["brutt√≥ √©rt√©k_nav", "brutt√≥ √©rt√©k Ft_nav"] if c in df_check.columns), None)
                netto_col  = next((c for c in ["nett√≥√©rt√©k_nav", "nett√≥√©rt√©k Ft_nav"] if c in df_check.columns), None)
                afa_col    = next((c for c in ["ad√≥√©rt√©k_nav", "ad√≥√©rt√©k Ft_nav"] if c in df_check.columns), None)
                
                # √ñsszegellen≈ërz√©sek
                df_check["Brutt√≥ egyezik?"] = df_check.apply(
                    lambda row: compare_with_tolerance(
                        normalize_number(row.get(brutto_col)) if brutto_col else None,
                        normalize_number(row.get("Brutt√≥ √°r_ai")),
                    ),
                    axis=1
                )
                df_check["Nett√≥ egyezik?"] = df_check.apply(
                    lambda row: compare_with_tolerance(
                        normalize_number(row.get(netto_col)) if netto_col else None,
                        normalize_number(row.get("Nett√≥ √°r_ai")),
                    ),
                    axis=1
                )
                df_check["√ÅFA egyezik?"] = df_check.apply(
                    lambda row: compare_with_tolerance(
                        normalize_number(row.get(afa_col)) if afa_col else None,
                        normalize_number(row.get("√ÅFA_ai")),
                    ),
                    axis=1
                )
                
                # Minden egyezik? logika:
                # - Ha mindh√°rom "Igen" ‚Üí ‚úÖ Igen
                # - Ha b√°rmelyik "Nem" ‚Üí ‚ùå Nem
                # - Egy√©bk√©nt (legal√°bb egy "Nincs adat", de nincs "Nem") ‚Üí Nincs adat
                def overall_match(row):
                    results = [row["Brutt√≥ egyezik?"], row["Nett√≥ egyezik?"], row["√ÅFA egyezik?"]]
                    if all(r == "Igen" for r in results):
                        return "‚úÖ Igen"
                    elif any(r == "Nem" for r in results):
                        return "‚ùå Nem"
                    else:
                        return "Nincs adat"
                
                df_check["Minden egyezik?"] = df_check.apply(overall_match, axis=1)
                
                # --- R√©szletez≈ë t√°bla ---
                df_details = pd.merge(
                    df_nav,   # NAV oszlopok _nav
                    df_gpt,   # GPT oszlopok _ai
                    how="right",
                    left_on="sz√°mlasorsz√°m_nav",
                    right_on="Sz√°mlasz√°m_ai"
                )
                
                # Sz√°mlaszint≈± ellen≈ërz√©sek visszacsatol√°sa
                df_details = pd.merge(
                    df_details,
                    df_check[["Sz√°mlasz√°m_ai", "Brutt√≥ egyezik?", "Nett√≥ egyezik?", "√ÅFA egyezik?", "Minden egyezik?"]],
                    how="left",
                    on="Sz√°mlasz√°m_ai"
                )
                
                # Ment√©s session_state-be
                st.session_state.df_merged_nav = df_details
                
                # Statisztika sz√°mlasz√°m szinten
                total = len(df_check)
                matched_all = (df_check["Minden egyezik?"] == "‚úÖ Igen").sum()
                match_rate = round(100 * matched_all / total, 2)
                
                st.session_state.stats_nav = {
                    "√ñsszes sz√°mla": total,
                    "Minden egyez√©s": matched_all,
                    "Teljes egyez√©si ar√°ny (%)": match_rate
                }
                
                st.success("‚úÖ NAV f√°jllal val√≥ √∂sszef≈±z√©s √©s ellen≈ërz√©s k√©sz!")

            except Exception as e:
                st.error(f"V√°ratlan hiba t√∂rt√©nt a NAV √∂sszef≈±z√©s sor√°n: {e}")

    if "df_merged_nav" in st.session_state:
        st.write("üìÑ **√ñsszef≈±z√∂tt √©s ellen≈ërz√∂tt t√°bl√°zat ‚Äì NAV (t√©telszinten):**")
        st.dataframe(make_arrow_compatible(st.session_state.df_merged_nav))

        # Excel let√∂lt√©s
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            st.session_state.df_merged_nav.to_excel(writer, sheet_name='NAV r√©szletek', index=False)

        st.download_button(
            label="üì• Let√∂lt√©s Excel (NAV r√©szletek)",
            data=buffer,
            file_name="merged_nav.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        st.markdown("### üìä Statisztika ‚Äì NAV √∂sszehasonl√≠t√°s")
        for k, v in st.session_state.stats_nav.items():
            st.write(f"**{k}:** {v}")


col_karton, col_mintanav = st.columns([1, 1])


with col_karton:
    st.subheader("üìé Kinyert adatok √∂sszef≈±z√©se: Karton")
    
    if st.button("üîó √ñsszef≈±z√©s a Kartonnal"):
        try:
            # GPT sz√°mlasz√°mok
            invoice_numbers = st.session_state.df_extracted["Sz√°mlasz√°m"].astype(str).unique()
    
            # Karton t√°bla el≈ëk√©sz√≠t√©se
            df_karton = st.session_state.df_karton.copy()
            df_karton.columns = [str(c).strip() for c in df_karton.columns]
    
            # Sz≈±r√©s: minden olyan sor kell, ahol b√°rmelyik oszlopban szerepel a sz√°mlasz√°m
            mask = df_karton.apply(lambda row: row.astype(str).isin(invoice_numbers).any(), axis=1)
            df_filtered_karton = df_karton[mask].copy()
    
            # Ha van "Bizonylat" vagy "Sz√°mlasz√°m" oszlop, rendezz√ºk arra
            for possible_col in ["Bizonylat", "Sz√°mlasz√°m", "sz√°mlasorsz√°m"]:
                if possible_col in df_filtered_karton.columns:
                    df_filtered_karton = df_filtered_karton.sort_values(by=possible_col)
                    break
    
            st.session_state.df_filtered_karton = df_filtered_karton
    
            # Statisztika: h√°ny GPT sz√°mlasz√°mhoz tal√°ltunk sorokat
            matched_karton = df_filtered_karton.apply(
                lambda row: any(str(val) in invoice_numbers for val in row.values), axis=1
            ).sum()
            total_karton = len(invoice_numbers)
    
            st.session_state.stats_karton = {
                "√ñsszes sz√°mla (GPT)": total_karton,
                "Kartonban megtal√°lt sorok": matched_karton,
            }
    
            st.success("‚úÖ Karton keres√©s √©s sz≈±r√©s k√©sz!")
    
        except Exception as e:
            st.error(f"‚ùå Hiba t√∂rt√©nt a Karton keres√©s sor√°n: {e}")
    
    if "df_filtered_karton" in st.session_state:
        st.write("üìÑ **Sz≈±rt t√°bl√°zat ‚Äì Karton (csak relev√°ns sorok):**")
        st.dataframe(make_arrow_compatible(st.session_state.df_filtered_karton))
    
        # Excel let√∂lt√©s el≈ëk√©sz√≠t√©s
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:
            st.session_state.df_filtered_karton.to_excel(writer, sheet_name="Karton sz≈±rt", index=False)
    
        st.download_button(
            label="üì• Let√∂lt√©s Excel (Karton)",
            data=buffer,
            file_name="filtered_karton.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    
        st.markdown("### üìä Statisztika ‚Äì Karton keres√©s")
        for k, v in st.session_state.stats_karton.items():
            st.write(f"**{k}:** {v}")
    
with col_mintanav:
    st.subheader("üìé Minta √©s NAV adatok √∂sszef≈±z√©se")

    if st.button("üîó √ñsszef≈±z√©s a Minta √©s NAV adatok k√∂z√∂tt"):
        missing = []
        if not df_ready(st.session_state.df_minta, required_cols=["Bizonylatsz√°m"]):
            missing.append("Mintav√©tel Excel")
        if not df_ready(st.session_state.df_nav, required_cols=["sz√°mlasorsz√°m"]):
            missing.append("NAV Excel")

        if missing:
            need_msg(missing)
        else:
            try:
                df_minta = st.session_state.df_minta.copy()
                df_nav = st.session_state.df_nav.copy()

                df_minta.columns = [str(c).strip() for c in df_minta.columns]
                df_nav.columns = [str(c).strip() for c in df_nav.columns]

                df_minta["Bizonylatsz√°m"] = df_minta["Bizonylatsz√°m"].astype(str)
                df_nav["sz√°mlasorsz√°m"] = df_nav["sz√°mlasorsz√°m"].astype(str)

                # add suffixes to avoid column name collisions
                df_minta_s = df_minta.add_suffix("_minta")
                df_nav_s = df_nav.add_suffix("_nav")

                # Left join (Minta as left)
                df_minta_nav = pd.merge(
                    df_minta_s,
                    df_nav_s,
                    how="left",
                    left_on="Bizonylatsz√°m_minta",
                    right_on="sz√°mlasorsz√°m_nav"
                )

                st.session_state.df_minta_nav = df_minta_nav

                # Simple stats
                total_rows = len(df_minta_nav)
                matched = df_minta_nav["sz√°mlasorsz√°m_nav"].notna().sum()
                unmatched = total_rows - matched
                match_rate = round(100 * matched / total_rows, 2) if total_rows else 0.0

                st.session_state.stats_minta_nav = {
                    "√ñsszes Minta sor": total_rows,
                    "NAV-ban tal√°lt sz√°ml√°k": matched,
                    "Hi√°nyz√≥ sz√°ml√°k": unmatched,
                    "Egyez√©si ar√°ny (%)": match_rate,
                }

                st.success("‚úÖ Minta √©s NAV adatok √∂sszef≈±z√©se k√©sz!")

            except Exception as e:
                st.error(f"‚ùå Hiba t√∂rt√©nt az √∂sszef≈±z√©s sor√°n: {e}")

    if "df_minta_nav" in st.session_state:
        st.write("üìÑ **√ñsszef≈±z√∂tt t√°bl√°zat ‚Äì Minta √ó NAV:**")
        st.dataframe(make_arrow_compatible(st.session_state.df_minta_nav))

        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:
            st.session_state.df_minta_nav.to_excel(writer, sheet_name="Minta_NAV", index=False)
        buffer.seek(0)

        st.download_button(
            label="üì• Let√∂lt√©s Excel (Minta‚ÄìNAV)",
            data=buffer,
            file_name="merged_minta_nav.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        st.markdown("### üìä Statisztika ‚Äì Minta √ó NAV √∂sszef≈±z√©s")
        for k, v in st.session_state.stats_minta_nav.items():
            st.write(f"**{k}:** {v}")




local_test = r"""

os.listdir('./')


import tomllib

secrets_path = r"C:\Users\abele\.streamlit\secrets.toml"

with open(secrets_path, "rb") as f:
    secrets = tomllib.load(f)

openai.api_key = secrets["OPENAI_API_KEY"]

MODEL = "gpt-4o"  # cheapest useful model
PDF_FILE = "9601013661.pdf"  # <-- replace with your own file path


with open(PDF_FILE, "rb") as f:
    text = extract_text_from_pdf(f)


# 2) GPT extraction
rows, tokens_used = extract_data_with_gpt(PDF_FILE, text, MODEL)


# 3) Create DataFrame
df = pd.DataFrame(rows, columns=[
    "F√°jl", "Elad√≥", "Vev≈ë", "Sz√°mlasz√°m", "Sz√°mla d√°tum",
    "Brutt√≥ √°r", "Nett√≥ √°r", "√ÅFA", "P√©nznem", "√Årfolyam"
])





"""



local_test = r"""
df_extracted = pd.read_csv('extract_data.csv')
df_minta = pd.read_excel('Mintav√©tel_k√∂lts√©gek_Sonneveld √©s ellen√Ørz√©s.xlsx', sheet_name='Mintav√©tel', skiprows = range(1, 9))
df_minta.columns = list(df_minta.iloc[0])
df_minta = df_minta.iloc[1:]
df_minta["Bizonylatsz√°m"] = df_minta["Bizonylatsz√°m"].astype(str)
df_karton = pd.read_excel('K√∂nyvel√©si karton 2024_Sonneveld Kft.xlsx', sheet_name='Munka1')

df_temp = pd.merge(df_minta, df_extracted, how='outer', left_on='Bizonylatsz√°m', right_on='Sz√°mlasz√°m')
nr_of_columns = len(df_temp.columns)

df_temp = pd.merge(df_temp, df_karton, how='left', left_on='Bizonylatsz√°m', right_on='Bizonylat')

column_to_compare = 'Bizonylatsz√°m'
columns_to_delete = df_temp.columns[:nr_of_columns]
df_merged = replace_successive_duplicates(df_temp, column_to_compare, columns_to_delete)

"""
